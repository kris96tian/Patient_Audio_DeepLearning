{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3543344,"sourceType":"datasetVersion","datasetId":2130795}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchaudio\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nimport os\nfrom torchvision import transforms\nimport torchaudio.transforms as T\nfrom pathlib import Path\n\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import Subset\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-10T13:58:34.531983Z","iopub.execute_input":"2024-09-10T13:58:34.532527Z","iopub.status.idle":"2024-09-10T13:58:34.540737Z","shell.execute_reply.started":"2024-09-10T13:58:34.532481Z","shell.execute_reply":"2024-09-10T13:58:34.539081Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"class VocalDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = Path(root_dir)\n        self.transform = transform\n        self.classes = ['Laryngozele', 'Normal', 'Vox senilis']\n        self.files = []\n        for label in self.classes:\n            class_path = self.root_dir / label\n            self.files += [(f, label) for f in class_path.glob('*.wav')]\n        # MelSpectrogram transformation\n        self.feature_transform = T.MelSpectrogram()\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        file_path, label = self.files[idx]\n        waveform, sample_rate = torchaudio.load(file_path)\n        \n        if waveform.size(0) > 1:\n            waveform = torch.mean(waveform, dim=0, keepdim=True)\n        \n        features = self.feature_transform(waveform)\n\n        target_length = 128  \n\n        # ? correct dimensions\n        if features.size(2) < target_length:\n            pad = target_length - features.size(2)\n            features = torch.nn.functional.pad(features, (0, pad))\n        else:\n            features = features[:, :, :target_length]\n\n        if self.transform:\n            features = self.transform(features)\n        \n        label_idx = self.classes.index(label)  \n        return features, label_idx\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:44:10.078774Z","iopub.execute_input":"2024-09-10T13:44:10.079320Z","iopub.status.idle":"2024-09-10T13:44:10.091925Z","shell.execute_reply.started":"2024-09-10T13:44:10.079274Z","shell.execute_reply":"2024-09-10T13:44:10.090272Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# initing dataset and check feature tensor shape\nroot_dir = \"/kaggle/input/patient-health-detection-using-vocal-audio/patient-vocal-dataset/patient-vocal-dataset\"\nvocal_dataset = VocalDataset(root_dir=root_dir)\n\nsample_waveform, _ = vocal_dataset[0]  \nprint(f\"Sample waveform shape: {sample_waveform.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:44:20.679004Z","iopub.execute_input":"2024-09-10T13:44:20.679599Z","iopub.status.idle":"2024-09-10T13:44:20.713837Z","shell.execute_reply.started":"2024-09-10T13:44:20.679541Z","shell.execute_reply":"2024-09-10T13:44:20.712539Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Sample waveform shape: torch.Size([1, 128, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Initing the DataLoader\ndataloader = DataLoader(vocal_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: x)  # Adjust collate_fn if needed\n\n# Print dataset and dataloader info\nprint(f\"Total number of files in dataset: {len(vocal_dataset)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:45:07.073530Z","iopub.execute_input":"2024-09-10T13:45:07.074053Z","iopub.status.idle":"2024-09-10T13:45:07.082055Z","shell.execute_reply.started":"2024-09-10T13:45:07.074012Z","shell.execute_reply":"2024-09-10T13:45:07.080549Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Total number of files in dataset: 1036\n","output_type":"stream"}]},{"cell_type":"code","source":"input_size = 128 * 128  # 16384\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:45:56.191341Z","iopub.execute_input":"2024-09-10T13:45:56.191878Z","iopub.status.idle":"2024-09-10T13:45:56.198290Z","shell.execute_reply.started":"2024-09-10T13:45:56.191833Z","shell.execute_reply":"2024-09-10T13:45:56.196840Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(LSTMModel, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        # x shape: [batch_size, sequence_length, input_size]\n        out, _ = self.lstm(x)\n        out = self.fc(out[:, -1, :])\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:46:02.351312Z","iopub.execute_input":"2024-09-10T13:46:02.351783Z","iopub.status.idle":"2024-09-10T13:46:02.491758Z","shell.execute_reply.started":"2024-09-10T13:46:02.351739Z","shell.execute_reply":"2024-09-10T13:46:02.490021Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"\n# Parameters for K-Fold\nnum_folds = 5\nbatch_size = 32\nnum_epochs = 10\nlearning_rate = 0.001\n\n# K-Fold cross-validation\nkfold = KFold(n_splits=num_folds, shuffle=True)\n\n# indices for cross-validation splits\nindices = list(range(len(vocal_dataset)))\n\nfor fold, (train_indices, val_indices) in enumerate(kfold.split(indices)):\n    print(f\"Fold {fold+1}/{num_folds}\")\n    \n    # my subsets for training and validation\n    train_subset = Subset(vocal_dataset, train_indices)\n    val_subset = Subset(vocal_dataset, val_indices)\n    \n    # my DataLoaders\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n    \n    # init the model, criterion, and optimizer\n    model = LSTMModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for features, labels in train_loader:\n            features = features.view(features.size(0), -1, input_size)\n            optimizer.zero_grad()\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        epoch_loss = running_loss / len(train_loader)\n        accuracy = 100 * correct / total\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n    \n    # Validation loop\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        \n        for features, labels in val_loader:\n            features = features.view(features.size(0), -1, input_size)\n            outputs = model(features)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        val_accuracy = 100 * correct / total\n        print(f\"Validation Accuracy for Fold {fold+1}: {val_accuracy:.2f}%\")\n    \n    print(\"-\" * 50)\n\nprint(\"Cross-validation complete\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:58:50.871515Z","iopub.execute_input":"2024-09-10T13:58:50.872322Z","iopub.status.idle":"2024-09-10T14:15:21.310560Z","shell.execute_reply.started":"2024-09-10T13:58:50.872256Z","shell.execute_reply":"2024-09-10T14:15:21.309297Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Fold 1/5\nEpoch [1/10], Loss: 0.9089, Accuracy: 56.28%\nEpoch [2/10], Loss: 0.8272, Accuracy: 65.82%\nEpoch [3/10], Loss: 0.8129, Accuracy: 62.56%\nEpoch [4/10], Loss: 0.7965, Accuracy: 68.24%\nEpoch [5/10], Loss: 0.8053, Accuracy: 66.43%\nEpoch [6/10], Loss: 0.7922, Accuracy: 66.18%\nEpoch [7/10], Loss: 0.7985, Accuracy: 66.06%\nEpoch [8/10], Loss: 0.8022, Accuracy: 65.22%\nEpoch [9/10], Loss: 0.7885, Accuracy: 68.84%\nEpoch [10/10], Loss: 0.7796, Accuracy: 69.20%\nValidation Accuracy for Fold 1: 59.13%\n--------------------------------------------------\nFold 2/5\nEpoch [1/10], Loss: 0.9016, Accuracy: 52.96%\nEpoch [2/10], Loss: 0.8433, Accuracy: 61.28%\nEpoch [3/10], Loss: 0.8518, Accuracy: 62.61%\nEpoch [4/10], Loss: 0.8385, Accuracy: 59.47%\nEpoch [5/10], Loss: 0.8274, Accuracy: 64.54%\nEpoch [6/10], Loss: 0.8301, Accuracy: 63.45%\nEpoch [7/10], Loss: 0.8241, Accuracy: 65.26%\nEpoch [8/10], Loss: 0.8046, Accuracy: 66.83%\nEpoch [9/10], Loss: 0.8111, Accuracy: 65.02%\nEpoch [10/10], Loss: 0.8187, Accuracy: 64.90%\nValidation Accuracy for Fold 2: 73.43%\n--------------------------------------------------\nFold 3/5\nEpoch [1/10], Loss: 0.9096, Accuracy: 54.64%\nEpoch [2/10], Loss: 0.8643, Accuracy: 58.26%\nEpoch [3/10], Loss: 0.8596, Accuracy: 57.18%\nEpoch [4/10], Loss: 0.8446, Accuracy: 58.38%\nEpoch [5/10], Loss: 0.8460, Accuracy: 58.14%\nEpoch [6/10], Loss: 0.8336, Accuracy: 65.38%\nEpoch [7/10], Loss: 0.8317, Accuracy: 64.90%\nEpoch [8/10], Loss: 0.8167, Accuracy: 63.33%\nEpoch [9/10], Loss: 0.8181, Accuracy: 66.83%\nEpoch [10/10], Loss: 0.8216, Accuracy: 62.97%\nValidation Accuracy for Fold 3: 53.62%\n--------------------------------------------------\nFold 4/5\nEpoch [1/10], Loss: 0.9357, Accuracy: 51.39%\nEpoch [2/10], Loss: 0.8517, Accuracy: 58.14%\nEpoch [3/10], Loss: 0.8323, Accuracy: 64.66%\nEpoch [4/10], Loss: 0.8305, Accuracy: 65.26%\nEpoch [5/10], Loss: 0.8400, Accuracy: 62.00%\nEpoch [6/10], Loss: 0.8358, Accuracy: 65.02%\nEpoch [7/10], Loss: 0.8322, Accuracy: 64.17%\nEpoch [8/10], Loss: 0.8288, Accuracy: 65.74%\nEpoch [9/10], Loss: 0.8274, Accuracy: 66.34%\nEpoch [10/10], Loss: 0.8252, Accuracy: 66.71%\nValidation Accuracy for Fold 4: 64.25%\n--------------------------------------------------\nFold 5/5\nEpoch [1/10], Loss: 0.9055, Accuracy: 54.16%\nEpoch [2/10], Loss: 0.8441, Accuracy: 59.95%\nEpoch [3/10], Loss: 0.8275, Accuracy: 64.90%\nEpoch [4/10], Loss: 0.8101, Accuracy: 62.85%\nEpoch [5/10], Loss: 0.8045, Accuracy: 68.52%\nEpoch [6/10], Loss: 0.8058, Accuracy: 65.98%\nEpoch [7/10], Loss: 0.8203, Accuracy: 61.88%\nEpoch [8/10], Loss: 0.8098, Accuracy: 65.86%\nEpoch [9/10], Loss: 0.8226, Accuracy: 62.12%\nEpoch [10/10], Loss: 0.8025, Accuracy: 64.78%\nValidation Accuracy for Fold 5: 67.63%\n--------------------------------------------------\nCross-validation complete\n","output_type":"stream"}]}]}